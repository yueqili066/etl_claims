{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "\n",
    "input_dir = data_dir\n",
    "output_dir = (data_dir + '_master_'\n",
    "              + datetime.datetime.today().strftime('%Y%m%d') + '/'\n",
    "              )\n",
    "ipynb_name = 'data_master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output folder\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_notin(a, b):\n",
    "    # a: pandas.core.series.Series\n",
    "    # b: pandas.core.series.Series\n",
    "    c = np.empty(0)\n",
    "    for element in a.unique():\n",
    "        if element not in b.unique(): c = np.append(c,element)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path of the folder\n",
    "path =  data_dir                \n",
    "# advisable to use os.path.join as this makes concatenation OS independent\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only show the file names of csv files\n",
    "source_name = os.path.basename(path).split('_')[0]\n",
    "di_version = os.path.basename(path)\n",
    "file_names = [os.path.basename(x).strip('_bsuy50') for x in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/Network.csv',\n",
       " './data/ServiceArea.csv',\n",
       " './data/BusinessRules.csv',\n",
       " './data/Crosswalk2016.csv',\n",
       " './data/PlanAttributes.csv',\n",
       " './data/Crosswalk2015.csv',\n",
       " './data/Rate.csv',\n",
       " './data/BenefitsCostSharing.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Network.csv',\n",
       " 'ServiceArea.csv',\n",
       " 'BusinessRules.csv',\n",
       " 'Crosswalk2016.csv',\n",
       " 'PlanAttributes.csv',\n",
       " 'Crosswalk2015.csv',\n",
       " 'Rate.csv',\n",
       " 'BenefitsCostSharing.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This chunck is not necessary; we run this when we need to figure out which csv file is failed to be read\n",
    "error_list = []\n",
    "for f in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f,sep='|')\n",
    "    except Exception:\n",
    "        print('Error in reading', f)\n",
    "        error_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files.remove('/Users/yueqi/Desktop/work_core/standard_input_datasets_142/CHST08_a20220228_di/di20220301/CHST08_a20220228_di20220301/CL_SPHR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show the length of row and col for each csv file\n",
    "length_row = [len(pd.read_csv(f)) for f in all_files]\n",
    "length_col = [len(pd.read_csv(f).columns) for f in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_source = pd.DataFrame(columns=['source_table_name', 'source_dataset_codename',\n",
    "                                   'source_ncol','source_nrow','di_version',\n",
    "                                   'di_action','description','notes','epic_definition'])\n",
    "tab_source['source_table_name'] = file_names\n",
    "tab_source['source_dataset_codename']=source_name # use the raw data folder name \n",
    "tab_source['di_version']=di_version # date for doing the deidentification, if raw data, show raw data\n",
    "tab_source['source_ncol']=length_col\n",
    "tab_source['source_nrow']=length_row\n",
    "tab_source['description']=''\n",
    "tab_source['di_action']=''\n",
    "tab_source['notes']=''\n",
    "tab_source['epic_definition']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master = pd.DataFrame(columns=['source_table_name', 'source_col_name','source_col_num','sparsity','example_value_1',\n",
    "                              'example_value_2','example_value_3','example_value_4','example_value_5',\n",
    "                              'di_action','di_status','description','epic_definition'])\n",
    "\n",
    "for f in all_files:\n",
    "    temp = pd.DataFrame(columns=['source_table_name', 'source_col_name','source_col_num','sparsity','example_value_1',\n",
    "                                'example_value_2','example_value_3','example_value_4','example_value_5',\n",
    "                                'di_action','di_status','description','epic_definition'])\n",
    "    df_from_each_file = (pd.read_csv(f))\n",
    "    temp['source_col_name'] = df_from_each_file.columns.values\n",
    "    for i in temp['source_col_name']:\n",
    "        list_5 = df_from_each_file[i].value_counts(dropna=False).keys()[:5].tolist()\n",
    "        list_5[len(list_5):5] = [np.nan]* (5-len(list_5))\n",
    "        temp.loc[temp['source_col_name']==i,'example_value_1'] = list_5[0] \n",
    "        temp.loc[temp['source_col_name']==i,'example_value_2'] = list_5[1] \n",
    "        temp.loc[temp['source_col_name']==i,'example_value_3'] = list_5[2]\n",
    "        temp.loc[temp['source_col_name']==i,'example_value_4'] = list_5[3]\n",
    "        temp.loc[temp['source_col_name']==i,'example_value_5'] = list_5[4]\n",
    "        temp.loc[temp['source_col_name']==i,'sparsity'] = ((df_from_each_file[i].isnull().sum()) / \n",
    "                        (df_from_each_file[i].size)).tolist()\n",
    "        temp.loc[temp['source_col_name']==i,'source_col_num'] = (df_from_each_file.columns.get_loc(i)) + 1\n",
    "    temp['source_table_name'] = os.path.basename(f).strip('_bsuy50')\n",
    "    master = master.append(temp,ignore_index = True, sort = False)\n",
    "# convert sparsity column into percentage\n",
    "master['sparsity'] = pd.Series([\"{0:.2f}%\".format(val * 100) for val in master['sparsity']], index = master.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_dir+'data_master.xlsx') as writer:\n",
    "    tab_source.to_excel(writer, sheet_name='source_tables',index = False,na_rep = 'NULL')\n",
    "    master.to_excel(writer, sheet_name='source_variables',index = False,na_rep = 'NULL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('etl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f861cf9a9f78d0157cc8b657bb40468b23c3c4b9f7d0c3ec262e4caa86ce2ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
